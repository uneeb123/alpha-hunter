<!--
  Template file for WebGL podcast video generation
  
  Template variables:
  - {{AUDIO_BASE64}}: Base64-encoded audio file
  - {{SUBTITLES_JSON}}: JSON array of subtitle objects with start, end, and text properties
-->
<!DOCTYPE html>
<html>
<head>
  <style>
    body, html { margin: 0; padding: 0; overflow: hidden; background: black; }
    #canvas { width: 720px; height: 1280px; display: block; }
    #subtitles { 
      position: absolute; 
      bottom: 200px; 
      width: 100%; 
      text-align: center; 
      color: white; 
      font-family: Arial; 
      font-size: 36px;
      text-shadow: 0 0 10px rgba(0,0,0,0.8);
      padding: 20px;
      box-sizing: border-box;
    }
  </style>
</head>
<body>
  <canvas id="canvas"></canvas>
  <div id="subtitles"></div>
  <audio id="audio" style="display:none">
    <source src="data:audio/mp3;base64,{{AUDIO_BASE64}}" type="audio/mp3">
  </audio>
  
  <script>
    // Subtitles data
    const subtitles = {{SUBTITLES_JSON}};
    const audio = document.getElementById('audio');
    const subtitlesEl = document.getElementById('subtitles');
    const canvas = document.getElementById('canvas');
    const gl = canvas.getContext('webgl');
    
    if (!gl) {
      console.error('WebGL not supported');
      document.body.innerHTML = 'WebGL not supported';
    }
    
    // Set up WebGL (simple black background with animated wave pattern)
    const vertexShaderSource = `
      attribute vec4 a_position;
      void main() {
        gl_Position = a_position;
      }
    `;
    
    const fragmentShaderSource = `
      precision mediump float;
      uniform float u_time;
      
      void main() {
        vec2 uv = gl_FragCoord.xy / vec2(720.0, 1280.0);
        
        // Create a subtle wave pattern
        float wave = sin(uv.y * 20.0 + u_time * 0.5) * 0.1;
        
        // Dark background with subtle blue/purple gradient
        vec3 color = mix(
          vec3(0.05, 0.05, 0.1),  // Dark purple-blue
          vec3(0.1, 0.1, 0.2),     // Slightly lighter blue
          uv.y + wave
        );
        
        gl_FragColor = vec4(color, 1.0);
      }
    `;
    
    // Create and compile shaders
    function createShader(gl, type, source) {
      const shader = gl.createShader(type);
      gl.shaderSource(shader, source);
      gl.compileShader(shader);
      
      if (!gl.getShaderParameter(shader, gl.COMPILE_STATUS)) {
        console.error('Shader compilation error:', gl.getShaderInfoLog(shader));
        gl.deleteShader(shader);
        return null;
      }
      
      return shader;
    }
    
    const vertexShader = createShader(gl, gl.VERTEX_SHADER, vertexShaderSource);
    const fragmentShader = createShader(gl, gl.FRAGMENT_SHADER, fragmentShaderSource);
    
    // Create and link program
    const program = gl.createProgram();
    gl.attachShader(program, vertexShader);
    gl.attachShader(program, fragmentShader);
    gl.linkProgram(program);
    
    if (!gl.getProgramParameter(program, gl.LINK_STATUS)) {
      console.error('Program linking error:', gl.getProgramInfoLog(program));
    }
    
    // Create a buffer with a rectangle covering the entire canvas
    const positionBuffer = gl.createBuffer();
    gl.bindBuffer(gl.ARRAY_BUFFER, positionBuffer);
    gl.bufferData(gl.ARRAY_BUFFER, new Float32Array([
      -1, -1,
       1, -1,
      -1,  1,
       1,  1
    ]), gl.STATIC_DRAW);
    
    // Set up attribute
    const positionAttributeLocation = gl.getAttribLocation(program, 'a_position');
    gl.enableVertexAttribArray(positionAttributeLocation);
    gl.vertexAttribPointer(positionAttributeLocation, 2, gl.FLOAT, false, 0, 0);
    
    // Get uniform locations
    const timeUniformLocation = gl.getUniformLocation(program, 'u_time');
    
    // Animation parameters
    let startTime = null;
    let animationFrameId = null;
    let capturing = false;
    let frameCount = 0;
    
    // Start audio and animation
    audio.addEventListener('canplaythrough', () => {
      // Wait a bit to ensure everything is ready
      setTimeout(() => {
        startCapture();
        audio.play();
      }, 1000);
    });
    
    // Update subtitles based on current time
    function updateSubtitles(time) {
      let currentSubtitle = '';
      
      for (const sub of subtitles) {
        if (time >= sub.start && time <= sub.end) {
          currentSubtitle = sub.text;
          break;
        }
      }
      
      subtitlesEl.textContent = currentSubtitle;
    }
    
    // Animation loop
    function render(timestamp) {
      if (!startTime) startTime = timestamp;
      const elapsedTime = (timestamp - startTime) / 1000; // convert to seconds
      
      // Update canvas size if needed
      if (canvas.width !== 720 || canvas.height !== 1280) {
        canvas.width = 720;
        canvas.height = 1280;
        gl.viewport(0, 0, canvas.width, canvas.height);
      }
      
      // Update subtitles based on audio time
      updateSubtitles(audio.currentTime);
      
      // Render WebGL frame
      gl.useProgram(program);
      gl.uniform1f(timeUniformLocation, elapsedTime);
      gl.drawArrays(gl.TRIANGLE_STRIP, 0, 4);
      
      // Capture frame if we're capturing
      if (capturing) {
        // Signal to puppeteer that we've rendered a frame
        window.captureFrame(frameCount, audio.currentTime, audio.duration);
        frameCount++;
      }
      
      // Continue animation if audio is still playing
      if (!audio.ended) {
        animationFrameId = requestAnimationFrame(render);
      } else {
        window.animationComplete();
      }
    }
    
    function startCapture() {
      capturing = true;
      // Start animation loop
      animationFrameId = requestAnimationFrame(render);
    }
    
    // Expose functions for puppeteer to call
    window.startCapture = startCapture;
    window.stopCapture = function() {
      capturing = false;
      if (animationFrameId) {
        cancelAnimationFrame(animationFrameId);
      }
    };
  </script>
</body>
</html>